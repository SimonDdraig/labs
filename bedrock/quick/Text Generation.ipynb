{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:dodgerblue\">Text Generation in Bedrock</p>\n",
    "<hr style=\"border:1px dotted; color:floralwhite\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements for this lab\n",
    "*See <span style=\"color:gold\">Appendix</span> at the bottom of this lab to install requirements for this lab*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted\">\n",
    "<hr style=\"border:1px dotted;color:cadetblue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:cadetblue\">Import libraries and set clients</p>\n",
    "*If the following does not show the latest (installed or upgraded) boto3 (at least v1.29.6), restart the kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.35.74'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3, json\n",
    "\n",
    "# region_name is important especially if you auth via aws configure and it is set to a region where there is no Bedrock yet\n",
    "#define common vars\n",
    "myRegion='us-west-2'\n",
    "\n",
    "# get clients\n",
    "bedrockRun = boto3.client(service_name='bedrock-runtime', region_name=myRegion)\n",
    "\n",
    "boto3.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:cadetblue\">\n",
    "<hr style=\"border:1px dotted;color:crimson\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:crimson\">Create some methods to call</p>\n",
    "1. Method for processing a prompt\n",
    "2. Method to have a conversation\n",
    "3. <b>Don't forget if you modify this method, make sure you execute the cell</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:crimson\">This method processes a SINGLE prompt with the chosen foundation model</p>\n",
    "No cacheing of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes a single prompt\n",
    "def processPrompt(m, prompt, p, t, k, n):\n",
    "    # prep some vars\n",
    "    body=''\n",
    "    outputResponse='Model: {}\\n---------------------------------------\\n'.format(m)\n",
    "    accept='application/json'\n",
    "    contentType='application/json'\n",
    "    tk=2000\n",
    "\n",
    "    # which model are we calling\n",
    "    match m:\n",
    "        case 'ai21.j2-ultra-v1' | 'ai21.j2-mid-v1': \n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t, \n",
    "                \"topP\": p, \n",
    "                \"maxTokens\": tk,\n",
    "                \"numResults\": n,\n",
    "            })\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            # Amazon - Titan\n",
    "            # https://aws.amazon.com/bedrock/titan/\n",
    "            body = json.dumps({\n",
    "                \"inputText\": prompt,\n",
    "                \"textGenerationConfig\": {\n",
    "                    \"temperature\": t,\n",
    "                    \"topP\": p,\n",
    "                    \"maxTokenCount\": tk,\n",
    "                },\n",
    "            })\n",
    "        case 'cohere.command-text-v14':\n",
    "            # Cohere - Command\n",
    "            # https://cohere.com/\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t,\n",
    "                \"p\": p,\n",
    "                \"k\": k,\n",
    "                \"max_tokens\": tk,\n",
    "                \"num_generations\": n,\n",
    "            })\n",
    "        case 'mistral.mistral-7b-instruct-v0:2':\n",
    "            # Mistral - Mistral\n",
    "            # https://mistral.ai/\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t,\n",
    "                \"top_p\": p,\n",
    "                \"top_k\": k,\n",
    "                \"max_tokens\": tk,\n",
    "            })\n",
    "        case 'meta.llama2-13b-chat-v1':\n",
    "            # Meta - Llama 2\n",
    "            # https://llama.meta.com/\n",
    "            body = json.dumps({\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": t,\n",
    "                \"top_p\": p,\n",
    "                \"max_gen_len\": tk,\n",
    "            })\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            # Anthropic - Claude\n",
    "            # https://www.anthropic.com/\n",
    "            body=json.dumps({\n",
    "                \"prompt\": \"\\n\\nHuman:{}\\n\\nAssistant:\".format(prompt),\n",
    "                \"temperature\": t,\n",
    "                \"top_p\": p,\n",
    "                \"top_k\": k,\n",
    "                \"max_tokens_to_sample\": tk,\n",
    "            })  \n",
    "        case 'anthropic.claude-3-7-sonnet-20250219-v1:0':\n",
    "            # Anthropic - Claude 3.7 Sonnet\n",
    "            # https://www.anthropic.com/\n",
    "            body=json.dumps({\n",
    "                \"prompt\": \"\\n\\nHuman:{}\\n\\nAssistant:\".format(prompt),\n",
    "                \"temperature\": t,\n",
    "                \"top_p\": p,\n",
    "                \"top_k\": k,\n",
    "                \"max_tokens_to_sample\": tk,\n",
    "            })  \n",
    "\n",
    "    #invoke the model\n",
    "    response = bedrockRun.invoke_model(body=body, modelId=m, accept=accept, contentType=contentType)\n",
    "    responseBody = json.loads(response.get('body').read())\n",
    "\n",
    "    # get the response\n",
    "    match m:\n",
    "        case 'ai21.j2-ultra-v1' | 'ai21.j2-mid-v1':            \n",
    "            for response in range(n):\n",
    "                outputText = responseBody.get('completions')[response].get('data').get('text')\n",
    "                outputResponse += 'RESPONSE: {}\\n{}\\n---------------------------------------\\n'.format(response+1, outputText)\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            outputText = responseBody['results'][0]['outputText']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "        case 'cohere.command-text-v14':\n",
    "            for response in range(n):\n",
    "                outputText = responseBody['generations'][response]['text']\n",
    "                outputResponse += 'RESPONSE: {}\\n{}\\n---------------------------------------\\n'.format(response+1, outputText)\n",
    "        case 'mistral.mistral-7b-instruct-v0:2':\n",
    "            outputText = responseBody['outputs'][0]['text']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "        case 'meta.llama2-13b-chat-v1':\n",
    "            outputText = responseBody['generation']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            outputText = responseBody['completion']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "        case 'anthropic.claude-3-7-sonnet-20250219-v1:0':\n",
    "            outputText = responseBody['completion']\n",
    "            outputResponse += '1 RESPONSE Supported:\\n{}\\n---------------------------------------\\n'.format(outputText)\n",
    "\n",
    "    # return response\n",
    "    return outputResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:crimson\">This following methods create a chatbot with the chosen foundation model</p>\n",
    "Cacheing of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process chatbot conversation\n",
    "# langchain required as models are stateless\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def chatBot(m, p, t): # demo_chatbot\n",
    "    # prep some vars\n",
    "    tk = 1000\n",
    "\n",
    "    # following connections the FM to langchain\n",
    "    # dont need a prompt yet as we haven't started the conversation\n",
    "    match m:\n",
    "        case \"ai21.j2-ultra-v1\" | \"ai21.j2-mid-v1\":\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t, \n",
    "                    \"topP\": p,\n",
    "                    \"maxTokens\": tk,\n",
    "                },\n",
    "            )\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t,\n",
    "                    \"topP\": p,\n",
    "                    \"maxTokenCount\": tk,\n",
    "                },\n",
    "            )\n",
    "        case \"meta.llama2-13b-chat-v1\" | \"meta.llama2-70b-chat-v1\":\n",
    "            # Meta - Llama 2\n",
    "            # https://llama.meta.com/\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t,\n",
    "                    \"top_p\": p,\n",
    "                    \"max_gen_len\": tk,\n",
    "                },\n",
    "            )\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            # Anthropic - Claude\n",
    "            # https://www.anthropic.com/\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t,\n",
    "                    \"top_p\": p,\n",
    "                    \"max_tokens_to_sample\": tk,\n",
    "                    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "                },\n",
    "            )\n",
    "        case 'anthropic.claude-3-7-sonnet-20250219-v1:0':\n",
    "            # Anthropic - Claude 3.7 Sonnet\n",
    "            # https://www.anthropic.com/\n",
    "            cl_llm = Bedrock(\n",
    "                model_id=m,\n",
    "                client=bedrockRun,\n",
    "                model_kwargs={\n",
    "                    \"temperature\": t,\n",
    "                    \"top_p\": p,\n",
    "                    \"max_tokens_to_sample\": tk,\n",
    "                    \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "                },\n",
    "            )\n",
    "\n",
    "    return cl_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot memory to remember conversations\n",
    "def chatBotMemory(m, p, t): # demo_memory\n",
    "    llmData=chatBot(m, p, t)\n",
    "    memory = ConversationBufferMemory(llm=llmData, max_token_limit=5000)\n",
    "    return memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot conversation chain\n",
    "def chatBotChain(prompt, m, p, t, memory): # demo_conversation\n",
    "    llmChain=chatBot(m, p, t)\n",
    "    llmConversation=ConversationChain(llm=llmChain, memory=memory, verbose=True)\n",
    "\n",
    "    response=llmConversation.predict(input=prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process prompt suitable for the model\n",
    "def chatBotPrompt(prompt, m, useTemplate):\n",
    "    # provide a template to stop model from simulation Human replies and continuing the conversation on its own\n",
    "    # you can tell the model how to answer, eg you are a primary school teacher, or answer in French, etc\n",
    "    if useTemplate:\n",
    "        t='{who1}[INST]You are a helpful bot which answers one question at a time.[/INST]{who2}\\n{who1}{human}{who2}'\n",
    "    else:\n",
    "        t='{who1}{human}{who2}'\n",
    "\n",
    "    match m:\n",
    "        case \"ai21.j2-ultra-v1\" | \"ai21.j2-mid-v1\":\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            prompt=t.format(who1='', human=prompt, who2='')\n",
    "        case 'amazon.titan-text-express-v1':\n",
    "            # AI21 Labs - Jurassic-2 Ultra and Mid \n",
    "            # https://www.ai21.com/studio\n",
    "            prompt=t.format(who1='User: ', human=prompt, who2='\\nBot:')\n",
    "        case \"meta.llama2-13b-chat-v1\" | \"meta.llama2-70b-chat-v1\":\n",
    "            # Meta - Llama 2\n",
    "            # https://llama.meta.com/\n",
    "            prompt=t.format(who1='', human=prompt, who2='')\n",
    "        case 'anthropic.claude-v2:1':\n",
    "            # Anthropic - Claude\n",
    "            # https://www.anthropic.com/\n",
    "            prompt=t.format(who1='\\n\\nHuman: ', human=prompt, who2='\\n\\nAssistant:')\n",
    "        case 'anthropic.claude-3-7-sonnet-20250219-v1:0':\n",
    "            # Anthropic - Claude 3.7 Sonnet\n",
    "            # https://www.anthropic.com/\n",
    "            prompt=t.format(who1='\\n\\nHuman: ', human=prompt, who2='\\n\\nAssistant:')\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:crimson\">Now we can start using our models with prompts</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:crimson\">\n",
    "<hr style=\"border:1px dotted;color:lightblue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:lightblue\">Invoke a model with a single prompt</p>\n",
    "1. Specify properties\n",
    "2. Call the method\n",
    "3. Prompt engineering <a href='https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html'>help</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the prompt and params - note each model will have different ranges for its params\n",
    "prompt='You are a data engineer, write a prompt using best practice prompt engineering I can then give to a genAI image generation model to create a beautiful image of auckland new zealand.'\n",
    "t=0.7 # temperature: use a lower value to decrease randomness in the response\n",
    "p=0.7 # topP: use a lower value to ignore less probable options\n",
    "k=50 # topK: number of most-likely candidates that the model considers for the next token\n",
    "n=2 # numResults: how many responses do you want, not all models support multiple responses\n",
    "\n",
    "# which model do you want to use based on the match statement below\n",
    "u=7\n",
    "\n",
    "match u:\n",
    "    case -1: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Ultra\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-ultra-v1'\n",
    "    case -2: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Mid\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-mid-v1'\n",
    "    case 3:\n",
    "        # Amazon - Titan\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "        m='amazon.titan-text-express-v1'\n",
    "    case -4: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Cohere - Command\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-cohere-command.html\n",
    "        m='cohere.command-text-v14'\n",
    "    case 5:\n",
    "        # Mistral - Mistral\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-mistral.html\n",
    "        m='mistral.mistral-7b-instruct-v0:2'\n",
    "    case -6: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Meta - Llama 2\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html\n",
    "        m='meta.llama2-13b-chat-v1'\n",
    "    case 7:\n",
    "        # Anthropic - Claude\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html\n",
    "        m='anthropic.claude-v2:1'\n",
    "    case 8:\n",
    "        # Anthropic - Claude 3.7 Sonnet\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html\n",
    "        m='anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "\n",
    "o=processPrompt(m, prompt, p, t, k, n)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:lightblue\">\n",
    "<hr style=\"border:1px dotted;color:deeppink\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:deeppink\">Invoke a chatbot model with a conversational prompt</p>\n",
    "1. Specify properties\n",
    "2. Call the method\n",
    "3. Prompt engineering <a href='https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html'>help</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the prompt and params - note each model will have different ranges for its params\n",
    "prompt='hello'\n",
    "t=0 # temperature: use a lower value to decrease randomness in the response\n",
    "p=0.1 # topP: use a lower value to ignore less probable options\n",
    "k=50 # topK: number of most-likely candidates that the model considers for the next token\n",
    "\n",
    "# which model do you want to use based on the match statement below\n",
    "u=3\n",
    "\n",
    "match u:\n",
    "    case -1: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Ultra\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-ultra-v1'\n",
    "    case -2: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # AI21 Labs - Jurassic-2 Mid\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-jurassic2.html\n",
    "        m='ai21.j2-mid-v1'\n",
    "    case 3:\n",
    "        # Amazon - Titan\n",
    "        # property ranges: https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-titan-text.html\n",
    "        m='amazon.titan-text-express-v1'\n",
    "    case -4: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Meta - Llama 2\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-meta.html\n",
    "        m='meta.llama2-70b-chat-v1'\n",
    "    case -5: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Anthropic - Claude\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html\n",
    "        m='anthropic.claude-v2:1'\n",
    "    case 8: # NOTE NOT AVAILABLE FOR THIS LAB\n",
    "        # Anthropic - Claude 3.7 Sonnet\n",
    "        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html\n",
    "        m='anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "\n",
    "# format prompt suitable for conversation\n",
    "prompt=chatBotPrompt(prompt, m, True)\n",
    "print(\"YOUR PROMPT FORMATTED:\\n\" + prompt)\n",
    "# start chatbot\n",
    "memory=chatBotMemory(m, p, t)\n",
    "o=chatBotChain(prompt, m, p, t, memory)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation\n",
    "prompt=chatBotPrompt(\"How old is planet earth?\", m, False)\n",
    "o=chatBotChain(prompt, m, p, t, memory)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the conversation\n",
    "prompt=chatBotPrompt(\"How do you know that?\", m, False)\n",
    "o=chatBotChain(prompt, m, p, t, memory)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:crimson\">\n",
    "<hr style=\"border:1px dotted;color:lightgreen\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:lightskyblue\">Anthropic - Claude - Using the Claude Massage API - Latest Gen</p>\n",
    "1. Set up Bedrock request\n",
    "2. Set the controls (https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html)\n",
    "3. Invoke the model with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m systemPromptC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest. Your goal is to provide informative and substantive responses to queries while avoiding potential harms.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m messages \u001b[38;5;241m=\u001b[39m [user_message]\n\u001b[0;32m---> 27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclaudeMessageAPI\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbedrockRun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelIdC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystemPromptC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(response, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     30\u001b[0m text_value \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mclaudeMessageAPI\u001b[0;34m(bedrock_runtime, model_id, system_prompt, messages, max_tokens)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclaudeMessageAPI\u001b[39m(bedrock_runtime, model_id, system_prompt, messages, max_tokens):\n\u001b[1;32m      4\u001b[0m     body\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(\n\u001b[1;32m      5\u001b[0m         {\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock-2023-05-31\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         }  \n\u001b[1;32m     11\u001b[0m     )  \n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     response_body \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response_body\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the InvokeModel operation: Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn’t supported. Retry your request with the ID or ARN of an inference profile that contains this model."
     ]
    }
   ],
   "source": [
    "# NOTE NOT AVAILABLE FOR THIS LAB\n",
    "def claudeMessageAPI(bedrock_runtime, model_id, system_prompt, messages, max_tokens):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"system\": system_prompt,\n",
    "            \"messages\": messages\n",
    "        }  \n",
    "    )  \n",
    "\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "   \n",
    "    return response_body\n",
    "\n",
    "# https://docs.anthropic.com/claude/reference/claude-on-amazon-bedrock\n",
    "#modelIdC = 'anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "modelIdC = 'us.anthropic.claude-3-7-sonnet-20250219-v1:0'\n",
    "# Prompt with user turn only.\n",
    "user_message =  {\"role\": \"user\", \"content\": \"Tell me a story in approximately 200 words, about what life on Mars could be like.\"}\n",
    "systemPromptC = \"You are Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest. Your goal is to provide informative and substantive responses to queries while avoiding potential harms.\"\n",
    "messages = [user_message]\n",
    "\n",
    "response = claudeMessageAPI (bedrockRun, modelIdC, systemPromptC, messages, 2000)\n",
    "\n",
    "print(json.dumps(response, indent=4))\n",
    "text_value = response['content'][0]['text']\n",
    "print(text_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:coral\">\n",
    "<hr style=\"border:1px dotted;color:gold\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:gold\">Appendix - Install Requirements (macOS)</p>\n",
    "# Requirements for this lab\n",
    "\n",
    "#### <p style=\"color:deeppink\">- If you are running VSCode on a laptop, follow all of below.<br>- If you are running Jupyter inside an AWS Account, you just need to install langchain. See 2.1 below and skip everything else.</p>\n",
    "\n",
    "*Windows requirements will be similar, apart from Homebrew.*  \n",
    "* python must be installed on your client, and be at least v3.8\n",
    "  * 3.8 supports the latest release of boto3 which supports Bedrock  \n",
    "* boto3 must be installed on your client, and be at least v1.33.9  \n",
    "  * *Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.*\n",
    "  * https://boto3.amazonaws.com/v1/documentation/api/latest/index.html\n",
    "### <p style=\"color:gold\">1. Homebrew</p> \n",
    "If you haven't installed Homebrew, you can install it by running the following command here or in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">1. Python</p> \n",
    "Once Homebrew is installed, you can install Python using the following command  \n",
    "*check what you have before installing/upgrading*  \n",
    "*you will need to quit and restart vsCode to use python once installed (or updated)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 --version\n",
    "which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "brew install python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">2. boto3 and other Python requirements</p> \n",
    "*check what you have before installing/upgrading*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m pip show boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">2.1 Chatbot requirements</p> \n",
    "*<p style=\"color:deeppink\">If you are running a Jupyter in AWS Account, you ONLY need these 2 cells</p>*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">3. aws configure</p> \n",
    "*Configure aws configure with credentials, and a user that has all of the Bedrock IAM policies required*  \n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/security_iam_id-based-policy-examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"color:gold\">4. Request Bedrock model access</p> \n",
    "*You must request access to the models required, you may need to provide use case details before you are able to request*  \n",
    "*Make sure you request in the region you intend to use the models in, this lab is us-west-2*  \n",
    "https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/modelaccess  \n",
    "\n",
    "Models required in this lab:\n",
    "\n",
    "* See code above for use of models and what access to request\n",
    "\n",
    "#### Pricing\n",
    "*Use 6 characters per token as an approximation for the number of tokens.*  \n",
    "https://aws.amazon.com/bedrock/pricing/  \n",
    "https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-prepare.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px dotted;color:gold\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
